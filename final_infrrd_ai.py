# -*- coding: utf-8 -*-
"""Final_Infrrd_ai.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SawN0Jl4JOqLvZK-j9Ekym5_B473FxLu
"""

from google.colab import drive
drive.mount('/content/drive')

#/content/drive/My Drive/Infrrd.ai
cd /content/drive/My\ Drive/Infrrd.ai

ls

import tensorflow as tf
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

# Commented out IPython magic to ensure Python compatibility.
# Imports
import numpy as np
import pandas as pd
import pickle
import matplotlib.pyplot as plt
# %matplotlib inline

# Sklearn Imports
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import KFold
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
import xgboost as xgb

# Keras Imports
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.wrappers.scikit_learn import KerasClassifier
from keras.optimizers import Adam

# Checking the data before training the models
df_check = pd.read_csv("train.csv")
print(df_check.shape)
print(df_check.head(5))
print("*****")
print(df_check.tail(5))

df_check.describe()

"""As we can observe from df_check.describe(),

-> We have 250 features(independent variables) and one Ground Truth.

-> Here I'm Dropping the "id" colum as it won't contribute in learning.

-> The dataset contains negative values to so I'm using "StandardScaler" for   "Normalizing" the input data to the models.

-> Also, there's no Categorical feature in our dataset.
"""

# Lets check number of classes in our Ground Truth
y = df_check.iloc[:, df_check.shape[1]-1].values # Grabbing only the last column 
number_classes = np.unique(y).shape[0]
print("Number of unique classes in our dataset: ",number_classes)

# Lets check whether our dataset is balanced for all the classes
ret0, ret1, ret2, ret3, ret4 = [], [], [], [], []
i = 0
while i < y.shape[0]:
  if y[i] == 0: ret0.append(y[i])
  elif y[i] == 1: ret1.append(y[i])
  elif y[i] == 2: ret2.append(y[i])
  elif y[i] == 3: ret3.append(y[i])
  elif y[i] == 4: ret4.append(y[i])
  i += 1

print(i)
print("Class_0:",len(ret0),"Class_1:",len(ret1),"Class_2:",len(ret2),"Class_3:",
      len(ret3),"Class_4:",len(ret4))

"""The above observation concludes our dataset is well balanced for all the classes"""

np.random.seed(9) # To produce same result

"""I have experimented on Various models:

1) Logistic Regression

2) Support Vector Machine

3) XGBoost

4) K Nearest Neighbor

5) Artificial Neural Network with 2 hidden layers
"""

# Function of all the Models I have experimented on
def Logistic_Regression_model(X_train, y_train,X_val,y_val):
  
  LR = LogisticRegression()
  parameters = {'penalty': ['l2'], 'C' : [0.001, 0.01, 0.1, 1, 10, 100, 1000], 
                'multi_class' :['multinomial'], 
                'solver': ['saga','lbfgs','newton-cg']}
  LR = GridSearchCV(LR, param_grid = parameters ,cv=5)
  LR.fit(X_train, y_train)
  score_train_acc = LR.score(X_train, y_train)
  print("Training Accuracy", score_train_acc)
  score_val_acc = LR.score(X_val, y_val)
  print("Validation Accuracy", score_val_acc) 
  y_train_check = LR.predict(X_train)
  print("**** TRAINING INFORMATION ****")
  print("Classification Report and Confusion Matrix on Training")
  print(classification_report(y_train, y_train_check))
  print(confusion_matrix(y_train, y_train_check, labels=np.unique(y_train)))
  print("------------------------------")
  print("**** VALIDATION INFORMATION ****")
  y_pred_LR = LR.predict(X_val)
  print("Classification Report and Confusion Matrix on Validation")
  print(classification_report(y_val, y_pred_LR))
  print(confusion_matrix(y_val, y_pred_LR, labels=np.unique(y_pred_LR)))
  pkl_filename = "LR_pickle_model.pkl"
  with open(pkl_filename, 'wb') as file:
    pickle.dump(LR, file)
  return score_train_acc, score_val_acc

def SVM_model(X_train, y_train,X_val,y_val):
  svm = SVC(random_state = 47, C = 0.1, kernel='rbf', 
                  class_weight = 'balanced', decision_function_shape = "ovr")
  parameters = {'C':[0.1, 0.001, 1],'decision_function_shape':['ovo', 'ovr']}
  svm = GridSearchCV(svm, param_grid= parameters ,cv=5)
  svm.fit(X_train, y_train)
  score_train_acc = svm.score(X_train, y_train)
  print("Training Accuracy", score_train_acc)
  score_val_acc = svm.score(X_val, y_val)
  print("Validation Accuracy", score_val_acc) 
  y_train_check = svm.predict(X_train)
  print("**** TRAINING INFORMATION ****")
  print("Classification Report and Confusion Matrix on Training")
  print(classification_report(y_train, y_train_check))
  print(confusion_matrix(y_train, y_train_check, labels=np.unique(y_train)))
  print("------------------------------")
  print("**** VALIDATION INFORMATION ****")
  y_pred_SVM = svm.predict(X_val)
  print("Classification Report and Confusion Matrix on Validation")
  print(classification_report(y_val, y_pred_SVM))
  print(confusion_matrix(y_val, y_pred_SVM, labels=np.unique(y_val)))
  pkl_filename = "SVM_pickle_model.pkl"
  with open(pkl_filename, 'wb') as file:
    pickle.dump(svm, file)
  return score_train_acc, score_val_acc

def XGBoost_model(X_train, y_train,X_val,y_val):
  XGB = xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0.4,
              learning_rate=0.1, max_delta_step=0, max_depth=6,
              min_child_weight=3, missing=None, n_estimators=100, n_jobs=1,
              nthread=None, objective='multi:softmax', random_state=0,
              reg_alpha=1e-05, reg_lambda=1, scale_pos_weight=1, seed=2,
              silent=None, subsample=0.8, verbosity=1)
  XGB.fit(X_train, y_train)
  score_train_acc = XGB.score(X_train, y_train)
  print("Training Accuracy", score_train_acc)
  score_val_acc = XGB.score(X_val, y_val)
  print("Validation Accuracy", score_val_acc)  
  y_train_check = XGB.predict(X_train)
  print("**** TRAINING INFORMATION ****")
  print("Classification Report and Confusion Matrix on Training")
  print(classification_report(y_train, y_train_check))
  print(confusion_matrix(y_train, y_train_check, labels=np.unique(y_train)))
  print("------------------------------")
  print("**** VALIDATION INFORMATION ****")
  y_pred_XGB = XGB.predict(X_val)
  print("Classification Report and Confusion Matrix on Validation")
  print(classification_report(y_val, y_pred_XGB))
  print(confusion_matrix(y_val, y_pred_XGB, labels=np.unique(y_pred_XGB)))
  pkl_filename = "XGB_pickle_model.pkl"
  with open(pkl_filename, 'wb') as file:
    pickle.dump(XGB, file)
  return score_train_acc, score_val_acc

def KNN_model(X_train, y_train,X_val,y_val):
  knn = KNeighborsClassifier(n_neighbors=5,weights='uniform',algorithm='auto')
  knn.fit(X_train, y_train)
  score_train_acc = knn.score(X_train, y_train)
  print("Training Accuracy", score_train_acc) 
  score_val_acc = knn.score(X_val, y_val)
  print("Validation Accuracy", score_val_acc)
  y_train_check = knn.predict(X_train)
  print("**** TRAINING INFORMATION ****")
  print("Classification Report and Confusion Matrix on Training")
  print(classification_report(y_train, y_train_check))
  print(confusion_matrix(y_train, y_train_check, labels=np.unique(y_train)))
  print("------------------------------")
  print("**** VALIDATION INFORMATION ****")
  y_pred_knn = knn.predict(X_val) 
  print("Classification Report and Confusion Matrix on Validation")
  print(classification_report(y_val, y_pred_knn))
  print(confusion_matrix(y_val, y_pred_knn, labels=np.unique(y_pred_knn)))
  pkl_filename = "KNN_pickle_model.pkl"
  with open(pkl_filename, 'wb') as file:
    pickle.dump(knn, file)
  return score_train_acc, score_val_acc

def ANN(learning_rate):
  adam = Adam(lr = learning_rate)
  classifier = Sequential()
  classifier.add(Dense(units = 128, activation = 'relu', 
                       kernel_initializer = 'random_uniform', input_dim = 250 ))
  classifier.add(Dropout(0.4))
  classifier.add(Dense(units = 128, activation = 'relu', 
                       kernel_initializer = 'random_uniform'))
  classifier.add(Dropout(0.4))
  classifier.add(Dense(units = 5, activation = 'softmax', 
                       kernel_initializer = 'random_uniform'))
  classifier.compile(optimizer = adam, loss = 'sparse_categorical_crossentropy',
                     metrics = ['sparse_categorical_accuracy'])
  print(classifier.summary())
  return classifier

def train_val_model(models):
  print("The model user have provided for tarining:", models)
  df = pd.read_csv("train.csv") # Creating DataFrame
  print("DataFrame",df.shape) # Checking the dimension
  X = df.iloc[:,1:-1].values # Converting DataFrame to Numpy and ignoring "id"
  print("X",X.shape) # Checking the dimension
  y = df.iloc[:, df.shape[1]-1].values # Converting DataFrame to Numpy for Ground Truth
  print(y.shape) # Checking the dimension
  # Normalizing the inpu
  sc = StandardScaler() 
  X = sc.fit_transform(X) 
  learning_rate = 0.0001  # Used for Adam Optimizer
  iterr = 100 # Number of Epochs
  n_folds = 5 # Number of folds for Cross Validation
  bsize = 32  # Batch Size
  train_acc, val_acc = [], [] # Lists for storing accuracies on each fold

  # Cross validation
  kfold = KFold(n_splits=n_folds, shuffle=True, random_state=7)
  hist = []
  val_hist=[]
  acc=[]
  val_acc=[]
  for train, val in kfold.split(X, y):
    X_train, X_val = X[train], X[val] # Grabbing X_train, X_val for Models
    y_train, y_val = y[train], y[val] # Grabbiing y_train, y_val for Models
    if models == "ANN":
        ann_model = ANN(learning_rate)
        history = ann_model.fit(X_train, y_train,validation_data=(X_val,y_val), epochs=iterr, batch_size=bsize, verbose=1)
        #print(history.history.keys())
        hist.append(history.history['loss'])
        val_hist.append(history.history['val_loss'])
        acc.append(history.history['sparse_categorical_accuracy'])
        val_acc.append(history.history['val_sparse_categorical_accuracy'])
        plt.figure()
        plt.subplot(121)
        # For Plotting, X_axis: Number of Epochs, Y_axis = Loss
        plt.title("Trainig Loss")
        plt.plot(history.history['loss'])
        plt.subplot(122)
        plt.title("Validation Loss")
        plt.plot(history.history['val_loss'])
        plt.show()
        yield hist,val_hist,acc,val_acc

    elif models == "LR":
        lr_model = Logistic_Regression_model(X_train, y_train,X_val,y_val)
        train_acc.append(lr_model[0])
        val_acc.append(lr_model[1])
        #print("TA",train_acc)
        print("Training Accuracy is:", sum(train_acc)/n_folds)
        print("Validation Accuracy is:", sum(val_acc)/n_folds)

    elif models == "SVM":
        svm_model = SVM_model(X_train, y_train,X_val,y_val)
        train_acc.append(svm_model[0])
        val_acc.append(svm_model[1])
        #print("TA",train_acc)
        print("Train Accuracy is:", sum(train_acc)/n_folds)
        print("Validation Accuracy is:", sum(val_acc)/n_folds)

    elif models == "XGB":
        xgb_model = XGBoost_model(X_train, y_train,X_val,y_val)
        train_acc.append(xgb_model[0])
        val_acc.append(xgb_model[1])
        #print("TA",train_acc)
        print("Train Accuracy is:", sum(train_acc)/n_folds)
        print("Validation Accuracy is:", sum(val_acc)/n_folds)

    elif models == "KNN":
        knn_model = KNN_model(X_train, y_train,X_val,y_val)
        train_acc.append(knn_model[0])
        val_acc.append(knn_model[1])
        #print("TA",train_acc)
        print("Train Accuracy is:", sum(train_acc)/n_folds)
        print("Validation Accuracy is:", sum(val_acc)/n_folds)

"""**Information on Training Models**

-> I'm using KFold Crossvalidation and splitting the Trainig data into Training and Validation

-> I have used GridSearchCV for best params

-> Also, I have **Hypertuned** all the experimented models and observed accuracy, loss, precision, recall, f1_scores

-> After observing all the experimented models I decided to use one model for Prediction as it has the least overfiting amongst all
"""

if __name__ == "__main__":
  model = "KNN" # Depends on User, User can select models to experiment from the given list ["LR","SVM","XGB","KNN","ANN"]
  if model == "ANN":
    try:
      hist,val_hist,acc,val_acc = train_val_model(models=model)
    except Exception as e:
      pass
  else:
    _ = train_val_model(models=model)
    for el in _: print(el)

"""**Finalizing Model**

**Results**

**Logistic Regression**

Training Accuracy: 0.7929166666666667 

Validation Accuracy is: 0.7455555555555555


**SVM**

Training Accuracy: 0.9930902777777778 

Validation Accuracy is: 0.9823611111111111

**XGB**

Training Accuracy: 0.1 

Validation Accuracy is: 0.8411

**KNN**

Training Accuracy: 0.9680208333333333 

Validation Accuracy is: 0.9330555555555555

**My Model**

Training Accuracy: 0.98834 

Validation Accuracy is: 0.96058

-> After considering various factors as mentioned previously I used K Nearest Neighbors for predicting the ground truth on Test data
"""

# Finalised Model

# Training DataFrame
df = pd.read_csv("train.csv")
print("DataFrame",df.shape)
X = df.iloc[:,1:-1].values
print("X_shape",X.shape)
y = df.iloc[:, df.shape[1]-1].values

# Testing DataFrame
df_test = pd.read_csv("test.csv")

ret_X = df_test.iloc[:,0].values # This will be used for Prediction.csv
#print(ret_X)
print("df_test",df_test.shape)

X_test = df_test.iloc[:,1:].values
print("X_test",X_test.shape)

# Normalizing the Data
sc = StandardScaler()
X_test = sc.fit_transform(X_test)
X = sc.fit_transform(X)

"""Use the following two consecutive blocks only if the trained model is not saved on the disck.

I faced this problem when I was working on the Jupyter Notebook.

Savig the model on Google Colab has no issues and you can directly dp prediction
"""

# Only use when model is not being saved
"""
def KNN_model(X_train, y_train,X_val,y_val, X_test):
  knn = KNeighborsClassifier(n_neighbors=5,weights='uniform',algorithm='auto')
  knn.fit(X_train, y_train)
  score_train_acc = knn.score(X_train, y_train)
  print("Training Accuracy", score_train_acc)
  score_val_acc = knn.score(X_val, y_val)
  print("Validating Accuracy", score_val_acc)
  y_pred_test = knn.predict(X_test)
  return y_pred_test"""

"""
kfold = KFold(n_splits=5, shuffle=True, random_state=7)
for train, val in kfold.split(X, y):
    X_train, X_val = X[train], X[val]
    y_train, y_val = y[train], y[val]
    
res = KNN_model(X_train, y_train,X_val,y_val, X_test)
print(len(res)) # Sanity check to match test input data rows"""

#np.unique(res).shape[0] # Checking number of unique predicted class

#ret = np.column_stack((ret_X, res)) # Combining the "id" column and "predicted class" for Prediction.csv

print(ret)

# Saving the Prediction.csv  
#pd.DataFrame(ret).to_csv("prediction.csv", sep = "|", header = ["id", "Predictions"], index = False)

# Load from saved model from file
with open("KNN_pickle_model.pkl", 'rb') as file:
    pickle_model = pickle.load(file)

y_predictions = pickle_model.predict(X_test)

np.unique(y_predictions).shape[0]

ret = np.column_stack((ret_X, y_predictions))

print(ret)
print(len(ret))

# Saving the Prediction.csv  
pd.DataFrame(ret).to_csv("Predictions.csv", sep = "|", header = ["id", "Predictions"], index = False)