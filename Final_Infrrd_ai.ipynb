{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "7buK-fcUX9x-",
    "outputId": "e3a1ee06-80b3-4822-f643-f71182be0315"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jWkmHYK3wI9f",
    "outputId": "4e4e8447-4afc-4406-d7df-7add9b28cc24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Infrrd.ai\n"
     ]
    }
   ],
   "source": [
    "#/content/drive/My Drive/Infrrd.ai\n",
    "cd /content/drive/My\\ Drive/Infrrd.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "kcDBP7dcgF-e",
    "outputId": "8355bd4e-65af-4589-a55d-d49378015d8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final_Infrrd_ai.ipynb  prediction.csv  train.csv\n",
      "Infrrd_ai.ipynb        test.csv        Untitled0.ipynb\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 79
    },
    "colab_type": "code",
    "id": "16Evdrm_Y3Bu",
    "outputId": "98d31da1-132f-48f0-c9bb-e08847a04e77"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PWJqJ85HY5Lf"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zOEwTYYbaFOJ"
   },
   "outputs": [],
   "source": [
    "# Sklearn Imports\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "32knOQGratO7",
    "outputId": "4e0519f0-5e45-497f-916f-a5c3ac0efaca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Keras Imports\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "id": "ouOVYzV39-eT",
    "outputId": "ceb5112f-48b4-4693-ff8f-50fadb0e0a4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7200, 252)\n",
      "   id   feature0   feature1  ...  feature248  feature249  label\n",
      "0   0  25.208249 -16.745748  ...  130.068955   -8.231081      1\n",
      "1   1 -86.931450   0.428227  ...  -26.164277   -4.909740      2\n",
      "2   2  42.160934   7.857013  ...  120.510705    7.754377      0\n",
      "3   3  20.666944   8.680642  ...  -40.426248    5.557970      1\n",
      "4   4  35.946679   4.573736  ...  -23.468499  -11.269110      0\n",
      "\n",
      "[5 rows x 252 columns]\n",
      "*****\n",
      "        id    feature0   feature1  ...  feature248  feature249  label\n",
      "7195  7195  134.810289   4.051190  ...   29.500229    1.698117      4\n",
      "7196  7196  124.446148   7.365961  ...   34.516724   18.481692      0\n",
      "7197  7197  -29.478376  -4.853181  ...  -48.317597    1.550521      4\n",
      "7198  7198   -7.350772 -13.460536  ...  -75.159849   -0.154655      3\n",
      "7199  7199  -57.925764  -4.437577  ...  -69.663835   15.484705      1\n",
      "\n",
      "[5 rows x 252 columns]\n"
     ]
    }
   ],
   "source": [
    "# Checking the data before training the models\n",
    "df_check = pd.read_csv(\"train.csv\")\n",
    "print(df_check.shape)\n",
    "print(df_check.head(5))\n",
    "print(\"*****\")\n",
    "print(df_check.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "colab_type": "code",
    "id": "KJmnEj1P-civ",
    "outputId": "b4cc70bc-6161-449e-f9cc-cc7a542e480e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>feature0</th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "      <th>feature11</th>\n",
       "      <th>feature12</th>\n",
       "      <th>feature13</th>\n",
       "      <th>feature14</th>\n",
       "      <th>feature15</th>\n",
       "      <th>feature16</th>\n",
       "      <th>feature17</th>\n",
       "      <th>feature18</th>\n",
       "      <th>feature19</th>\n",
       "      <th>feature20</th>\n",
       "      <th>feature21</th>\n",
       "      <th>feature22</th>\n",
       "      <th>feature23</th>\n",
       "      <th>feature24</th>\n",
       "      <th>feature25</th>\n",
       "      <th>feature26</th>\n",
       "      <th>feature27</th>\n",
       "      <th>feature28</th>\n",
       "      <th>feature29</th>\n",
       "      <th>feature30</th>\n",
       "      <th>feature31</th>\n",
       "      <th>feature32</th>\n",
       "      <th>feature33</th>\n",
       "      <th>feature34</th>\n",
       "      <th>feature35</th>\n",
       "      <th>feature36</th>\n",
       "      <th>feature37</th>\n",
       "      <th>feature38</th>\n",
       "      <th>...</th>\n",
       "      <th>feature211</th>\n",
       "      <th>feature212</th>\n",
       "      <th>feature213</th>\n",
       "      <th>feature214</th>\n",
       "      <th>feature215</th>\n",
       "      <th>feature216</th>\n",
       "      <th>feature217</th>\n",
       "      <th>feature218</th>\n",
       "      <th>feature219</th>\n",
       "      <th>feature220</th>\n",
       "      <th>feature221</th>\n",
       "      <th>feature222</th>\n",
       "      <th>feature223</th>\n",
       "      <th>feature224</th>\n",
       "      <th>feature225</th>\n",
       "      <th>feature226</th>\n",
       "      <th>feature227</th>\n",
       "      <th>feature228</th>\n",
       "      <th>feature229</th>\n",
       "      <th>feature230</th>\n",
       "      <th>feature231</th>\n",
       "      <th>feature232</th>\n",
       "      <th>feature233</th>\n",
       "      <th>feature234</th>\n",
       "      <th>feature235</th>\n",
       "      <th>feature236</th>\n",
       "      <th>feature237</th>\n",
       "      <th>feature238</th>\n",
       "      <th>feature239</th>\n",
       "      <th>feature240</th>\n",
       "      <th>feature241</th>\n",
       "      <th>feature242</th>\n",
       "      <th>feature243</th>\n",
       "      <th>feature244</th>\n",
       "      <th>feature245</th>\n",
       "      <th>feature246</th>\n",
       "      <th>feature247</th>\n",
       "      <th>feature248</th>\n",
       "      <th>feature249</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3599.500000</td>\n",
       "      <td>2.504684</td>\n",
       "      <td>0.615413</td>\n",
       "      <td>3.016680</td>\n",
       "      <td>0.243273</td>\n",
       "      <td>-0.158600</td>\n",
       "      <td>0.506224</td>\n",
       "      <td>-0.276842</td>\n",
       "      <td>0.540687</td>\n",
       "      <td>0.608447</td>\n",
       "      <td>-0.216571</td>\n",
       "      <td>-0.640796</td>\n",
       "      <td>0.159247</td>\n",
       "      <td>-0.626838</td>\n",
       "      <td>0.242237</td>\n",
       "      <td>-0.119154</td>\n",
       "      <td>0.235442</td>\n",
       "      <td>-0.155578</td>\n",
       "      <td>-1.668230</td>\n",
       "      <td>-5.123835</td>\n",
       "      <td>2.364884</td>\n",
       "      <td>0.042340</td>\n",
       "      <td>-0.435367</td>\n",
       "      <td>0.541755</td>\n",
       "      <td>-1.373647</td>\n",
       "      <td>-0.547132</td>\n",
       "      <td>0.187925</td>\n",
       "      <td>-1.045961</td>\n",
       "      <td>-7.151048</td>\n",
       "      <td>0.547263</td>\n",
       "      <td>-0.597533</td>\n",
       "      <td>-0.151262</td>\n",
       "      <td>0.270463</td>\n",
       "      <td>0.165594</td>\n",
       "      <td>0.150636</td>\n",
       "      <td>-0.206447</td>\n",
       "      <td>0.189826</td>\n",
       "      <td>4.817891</td>\n",
       "      <td>0.390156</td>\n",
       "      <td>-0.325271</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.663008</td>\n",
       "      <td>-0.211821</td>\n",
       "      <td>-0.134939</td>\n",
       "      <td>0.636004</td>\n",
       "      <td>-0.121269</td>\n",
       "      <td>-0.221302</td>\n",
       "      <td>-0.388627</td>\n",
       "      <td>0.263071</td>\n",
       "      <td>-0.130988</td>\n",
       "      <td>-0.328374</td>\n",
       "      <td>0.191363</td>\n",
       "      <td>0.016726</td>\n",
       "      <td>0.531364</td>\n",
       "      <td>0.180472</td>\n",
       "      <td>0.186527</td>\n",
       "      <td>-0.686691</td>\n",
       "      <td>0.648639</td>\n",
       "      <td>-0.254978</td>\n",
       "      <td>-0.331176</td>\n",
       "      <td>-0.292446</td>\n",
       "      <td>-0.049037</td>\n",
       "      <td>-0.294538</td>\n",
       "      <td>-0.656949</td>\n",
       "      <td>-0.360518</td>\n",
       "      <td>-0.557920</td>\n",
       "      <td>4.276076</td>\n",
       "      <td>0.197867</td>\n",
       "      <td>-0.154491</td>\n",
       "      <td>-0.582435</td>\n",
       "      <td>-0.160002</td>\n",
       "      <td>0.605181</td>\n",
       "      <td>-0.020437</td>\n",
       "      <td>-0.958585</td>\n",
       "      <td>-0.260203</td>\n",
       "      <td>-4.950015</td>\n",
       "      <td>-0.159411</td>\n",
       "      <td>0.672097</td>\n",
       "      <td>3.745077</td>\n",
       "      <td>0.273164</td>\n",
       "      <td>2.009167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2078.605302</td>\n",
       "      <td>67.755261</td>\n",
       "      <td>8.231712</td>\n",
       "      <td>65.188459</td>\n",
       "      <td>8.518782</td>\n",
       "      <td>8.190667</td>\n",
       "      <td>8.356110</td>\n",
       "      <td>8.365744</td>\n",
       "      <td>67.375739</td>\n",
       "      <td>8.133119</td>\n",
       "      <td>8.138978</td>\n",
       "      <td>8.355347</td>\n",
       "      <td>8.040079</td>\n",
       "      <td>8.091278</td>\n",
       "      <td>8.245793</td>\n",
       "      <td>8.340967</td>\n",
       "      <td>8.139238</td>\n",
       "      <td>8.221563</td>\n",
       "      <td>60.821216</td>\n",
       "      <td>68.689017</td>\n",
       "      <td>69.986011</td>\n",
       "      <td>8.236550</td>\n",
       "      <td>8.071465</td>\n",
       "      <td>8.014914</td>\n",
       "      <td>70.386622</td>\n",
       "      <td>8.165660</td>\n",
       "      <td>8.316625</td>\n",
       "      <td>8.136781</td>\n",
       "      <td>70.588599</td>\n",
       "      <td>8.187827</td>\n",
       "      <td>8.238298</td>\n",
       "      <td>8.098474</td>\n",
       "      <td>8.065563</td>\n",
       "      <td>8.015513</td>\n",
       "      <td>8.091156</td>\n",
       "      <td>8.398740</td>\n",
       "      <td>8.317210</td>\n",
       "      <td>69.011938</td>\n",
       "      <td>69.203657</td>\n",
       "      <td>7.995532</td>\n",
       "      <td>...</td>\n",
       "      <td>8.361792</td>\n",
       "      <td>8.081083</td>\n",
       "      <td>8.149971</td>\n",
       "      <td>8.315094</td>\n",
       "      <td>8.057111</td>\n",
       "      <td>8.544333</td>\n",
       "      <td>8.187305</td>\n",
       "      <td>8.262458</td>\n",
       "      <td>8.165704</td>\n",
       "      <td>7.903199</td>\n",
       "      <td>8.141047</td>\n",
       "      <td>8.250105</td>\n",
       "      <td>8.249824</td>\n",
       "      <td>8.369027</td>\n",
       "      <td>8.073964</td>\n",
       "      <td>8.155860</td>\n",
       "      <td>8.246036</td>\n",
       "      <td>8.323372</td>\n",
       "      <td>8.122141</td>\n",
       "      <td>8.188542</td>\n",
       "      <td>8.405445</td>\n",
       "      <td>8.041267</td>\n",
       "      <td>8.163116</td>\n",
       "      <td>8.222834</td>\n",
       "      <td>8.125606</td>\n",
       "      <td>62.531184</td>\n",
       "      <td>8.195790</td>\n",
       "      <td>8.422409</td>\n",
       "      <td>8.236291</td>\n",
       "      <td>8.239107</td>\n",
       "      <td>8.200987</td>\n",
       "      <td>8.390035</td>\n",
       "      <td>64.426649</td>\n",
       "      <td>8.162079</td>\n",
       "      <td>65.950715</td>\n",
       "      <td>8.218073</td>\n",
       "      <td>8.067559</td>\n",
       "      <td>69.372084</td>\n",
       "      <td>8.282975</td>\n",
       "      <td>1.411923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-236.552276</td>\n",
       "      <td>-28.061843</td>\n",
       "      <td>-226.049557</td>\n",
       "      <td>-28.452153</td>\n",
       "      <td>-33.186906</td>\n",
       "      <td>-27.092009</td>\n",
       "      <td>-31.324360</td>\n",
       "      <td>-244.329485</td>\n",
       "      <td>-27.871326</td>\n",
       "      <td>-31.365588</td>\n",
       "      <td>-32.690045</td>\n",
       "      <td>-29.973536</td>\n",
       "      <td>-30.504274</td>\n",
       "      <td>-28.555918</td>\n",
       "      <td>-33.765700</td>\n",
       "      <td>-33.074933</td>\n",
       "      <td>-27.114766</td>\n",
       "      <td>-214.555481</td>\n",
       "      <td>-243.798895</td>\n",
       "      <td>-282.342354</td>\n",
       "      <td>-29.015075</td>\n",
       "      <td>-29.760087</td>\n",
       "      <td>-26.742305</td>\n",
       "      <td>-230.059512</td>\n",
       "      <td>-32.905233</td>\n",
       "      <td>-27.947689</td>\n",
       "      <td>-34.683619</td>\n",
       "      <td>-290.115260</td>\n",
       "      <td>-29.770034</td>\n",
       "      <td>-30.621127</td>\n",
       "      <td>-32.972128</td>\n",
       "      <td>-32.772882</td>\n",
       "      <td>-31.402202</td>\n",
       "      <td>-25.315229</td>\n",
       "      <td>-30.001914</td>\n",
       "      <td>-36.817586</td>\n",
       "      <td>-262.076661</td>\n",
       "      <td>-239.978368</td>\n",
       "      <td>-35.850329</td>\n",
       "      <td>...</td>\n",
       "      <td>-30.144300</td>\n",
       "      <td>-31.268302</td>\n",
       "      <td>-34.876253</td>\n",
       "      <td>-39.195365</td>\n",
       "      <td>-30.905763</td>\n",
       "      <td>-31.843946</td>\n",
       "      <td>-34.676757</td>\n",
       "      <td>-29.039543</td>\n",
       "      <td>-31.624231</td>\n",
       "      <td>-28.580088</td>\n",
       "      <td>-31.993894</td>\n",
       "      <td>-31.453285</td>\n",
       "      <td>-28.158385</td>\n",
       "      <td>-27.693307</td>\n",
       "      <td>-30.966224</td>\n",
       "      <td>-26.934303</td>\n",
       "      <td>-30.445080</td>\n",
       "      <td>-29.140377</td>\n",
       "      <td>-31.176162</td>\n",
       "      <td>-29.809755</td>\n",
       "      <td>-31.346080</td>\n",
       "      <td>-38.556724</td>\n",
       "      <td>-30.027046</td>\n",
       "      <td>-28.385724</td>\n",
       "      <td>-40.437589</td>\n",
       "      <td>-278.866018</td>\n",
       "      <td>-28.284312</td>\n",
       "      <td>-30.330342</td>\n",
       "      <td>-35.250707</td>\n",
       "      <td>-28.605101</td>\n",
       "      <td>-30.405404</td>\n",
       "      <td>-31.543493</td>\n",
       "      <td>-258.567376</td>\n",
       "      <td>-30.154402</td>\n",
       "      <td>-248.161392</td>\n",
       "      <td>-33.408119</td>\n",
       "      <td>-31.436556</td>\n",
       "      <td>-295.175132</td>\n",
       "      <td>-29.377652</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1799.750000</td>\n",
       "      <td>-42.590075</td>\n",
       "      <td>-5.004813</td>\n",
       "      <td>-39.828719</td>\n",
       "      <td>-5.587758</td>\n",
       "      <td>-5.582504</td>\n",
       "      <td>-5.227875</td>\n",
       "      <td>-5.904324</td>\n",
       "      <td>-45.382821</td>\n",
       "      <td>-4.945470</td>\n",
       "      <td>-5.644610</td>\n",
       "      <td>-6.104567</td>\n",
       "      <td>-5.143654</td>\n",
       "      <td>-6.087070</td>\n",
       "      <td>-5.424580</td>\n",
       "      <td>-5.749908</td>\n",
       "      <td>-5.358593</td>\n",
       "      <td>-5.766079</td>\n",
       "      <td>-42.625033</td>\n",
       "      <td>-52.417373</td>\n",
       "      <td>-44.563757</td>\n",
       "      <td>-5.437398</td>\n",
       "      <td>-6.038635</td>\n",
       "      <td>-4.773741</td>\n",
       "      <td>-49.945201</td>\n",
       "      <td>-6.168683</td>\n",
       "      <td>-5.392298</td>\n",
       "      <td>-6.481174</td>\n",
       "      <td>-53.705674</td>\n",
       "      <td>-4.983741</td>\n",
       "      <td>-6.172823</td>\n",
       "      <td>-5.659687</td>\n",
       "      <td>-5.088565</td>\n",
       "      <td>-5.350488</td>\n",
       "      <td>-5.398295</td>\n",
       "      <td>-5.841162</td>\n",
       "      <td>-5.378559</td>\n",
       "      <td>-40.684947</td>\n",
       "      <td>-46.968973</td>\n",
       "      <td>-5.702115</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.362336</td>\n",
       "      <td>-5.723578</td>\n",
       "      <td>-5.702435</td>\n",
       "      <td>-4.757658</td>\n",
       "      <td>-5.468284</td>\n",
       "      <td>-5.939967</td>\n",
       "      <td>-5.865435</td>\n",
       "      <td>-5.262297</td>\n",
       "      <td>-5.662243</td>\n",
       "      <td>-5.546078</td>\n",
       "      <td>-5.218620</td>\n",
       "      <td>-5.647246</td>\n",
       "      <td>-5.195219</td>\n",
       "      <td>-5.429177</td>\n",
       "      <td>-5.123447</td>\n",
       "      <td>-6.312873</td>\n",
       "      <td>-4.879428</td>\n",
       "      <td>-5.919964</td>\n",
       "      <td>-5.631948</td>\n",
       "      <td>-5.865277</td>\n",
       "      <td>-5.693332</td>\n",
       "      <td>-5.669535</td>\n",
       "      <td>-6.179590</td>\n",
       "      <td>-5.883235</td>\n",
       "      <td>-6.070847</td>\n",
       "      <td>-36.594083</td>\n",
       "      <td>-5.362105</td>\n",
       "      <td>-5.820926</td>\n",
       "      <td>-6.214888</td>\n",
       "      <td>-5.739435</td>\n",
       "      <td>-5.005272</td>\n",
       "      <td>-5.718761</td>\n",
       "      <td>-44.261020</td>\n",
       "      <td>-5.763305</td>\n",
       "      <td>-49.965317</td>\n",
       "      <td>-5.703261</td>\n",
       "      <td>-4.710237</td>\n",
       "      <td>-43.293031</td>\n",
       "      <td>-5.408092</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3599.500000</td>\n",
       "      <td>2.920268</td>\n",
       "      <td>0.562655</td>\n",
       "      <td>2.704124</td>\n",
       "      <td>0.292459</td>\n",
       "      <td>-0.117978</td>\n",
       "      <td>0.566189</td>\n",
       "      <td>-0.201625</td>\n",
       "      <td>0.954221</td>\n",
       "      <td>0.567095</td>\n",
       "      <td>-0.240709</td>\n",
       "      <td>-0.730223</td>\n",
       "      <td>0.172110</td>\n",
       "      <td>-0.500005</td>\n",
       "      <td>0.089438</td>\n",
       "      <td>0.039119</td>\n",
       "      <td>0.355361</td>\n",
       "      <td>-0.245847</td>\n",
       "      <td>-3.209224</td>\n",
       "      <td>-5.549804</td>\n",
       "      <td>1.852456</td>\n",
       "      <td>-0.056584</td>\n",
       "      <td>-0.374483</td>\n",
       "      <td>0.618238</td>\n",
       "      <td>-1.012249</td>\n",
       "      <td>-0.557661</td>\n",
       "      <td>0.092398</td>\n",
       "      <td>-1.137685</td>\n",
       "      <td>-7.047823</td>\n",
       "      <td>0.491090</td>\n",
       "      <td>-0.601863</td>\n",
       "      <td>-0.242165</td>\n",
       "      <td>0.222670</td>\n",
       "      <td>0.192796</td>\n",
       "      <td>0.059832</td>\n",
       "      <td>-0.115719</td>\n",
       "      <td>0.124860</td>\n",
       "      <td>4.726076</td>\n",
       "      <td>0.371974</td>\n",
       "      <td>-0.437882</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.636378</td>\n",
       "      <td>-0.247108</td>\n",
       "      <td>-0.185262</td>\n",
       "      <td>0.640615</td>\n",
       "      <td>-0.070084</td>\n",
       "      <td>-0.136546</td>\n",
       "      <td>-0.375165</td>\n",
       "      <td>0.307469</td>\n",
       "      <td>-0.195962</td>\n",
       "      <td>-0.331553</td>\n",
       "      <td>0.354485</td>\n",
       "      <td>0.103275</td>\n",
       "      <td>0.480724</td>\n",
       "      <td>0.114207</td>\n",
       "      <td>0.291601</td>\n",
       "      <td>-0.706724</td>\n",
       "      <td>0.740648</td>\n",
       "      <td>-0.238430</td>\n",
       "      <td>-0.356445</td>\n",
       "      <td>-0.383163</td>\n",
       "      <td>-0.089130</td>\n",
       "      <td>-0.258489</td>\n",
       "      <td>-0.624067</td>\n",
       "      <td>-0.485799</td>\n",
       "      <td>-0.471152</td>\n",
       "      <td>3.805869</td>\n",
       "      <td>0.148899</td>\n",
       "      <td>-0.097745</td>\n",
       "      <td>-0.636661</td>\n",
       "      <td>-0.258812</td>\n",
       "      <td>0.498952</td>\n",
       "      <td>0.011173</td>\n",
       "      <td>-0.394964</td>\n",
       "      <td>-0.401264</td>\n",
       "      <td>-5.736435</td>\n",
       "      <td>-0.094127</td>\n",
       "      <td>0.574117</td>\n",
       "      <td>5.055538</td>\n",
       "      <td>0.182123</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5399.250000</td>\n",
       "      <td>48.558923</td>\n",
       "      <td>5.964513</td>\n",
       "      <td>46.821744</td>\n",
       "      <td>5.858455</td>\n",
       "      <td>5.371252</td>\n",
       "      <td>6.195204</td>\n",
       "      <td>5.357205</td>\n",
       "      <td>46.362046</td>\n",
       "      <td>6.066210</td>\n",
       "      <td>5.228505</td>\n",
       "      <td>4.939052</td>\n",
       "      <td>5.644562</td>\n",
       "      <td>4.907172</td>\n",
       "      <td>5.850259</td>\n",
       "      <td>5.459680</td>\n",
       "      <td>5.834696</td>\n",
       "      <td>5.333978</td>\n",
       "      <td>38.305245</td>\n",
       "      <td>41.646553</td>\n",
       "      <td>49.297300</td>\n",
       "      <td>5.509753</td>\n",
       "      <td>4.952568</td>\n",
       "      <td>5.821988</td>\n",
       "      <td>46.004289</td>\n",
       "      <td>4.922412</td>\n",
       "      <td>5.643996</td>\n",
       "      <td>4.327633</td>\n",
       "      <td>39.661722</td>\n",
       "      <td>5.945666</td>\n",
       "      <td>4.945833</td>\n",
       "      <td>5.413526</td>\n",
       "      <td>5.684159</td>\n",
       "      <td>5.502697</td>\n",
       "      <td>5.593477</td>\n",
       "      <td>5.425138</td>\n",
       "      <td>5.714981</td>\n",
       "      <td>51.693433</td>\n",
       "      <td>46.950923</td>\n",
       "      <td>5.079953</td>\n",
       "      <td>...</td>\n",
       "      <td>4.867916</td>\n",
       "      <td>5.260688</td>\n",
       "      <td>5.333803</td>\n",
       "      <td>6.262940</td>\n",
       "      <td>5.292772</td>\n",
       "      <td>5.438497</td>\n",
       "      <td>5.110451</td>\n",
       "      <td>5.852768</td>\n",
       "      <td>5.294345</td>\n",
       "      <td>5.003673</td>\n",
       "      <td>5.632822</td>\n",
       "      <td>5.573464</td>\n",
       "      <td>6.012878</td>\n",
       "      <td>5.803996</td>\n",
       "      <td>5.758700</td>\n",
       "      <td>4.842543</td>\n",
       "      <td>6.185220</td>\n",
       "      <td>5.346402</td>\n",
       "      <td>5.128535</td>\n",
       "      <td>5.195492</td>\n",
       "      <td>5.676939</td>\n",
       "      <td>5.216738</td>\n",
       "      <td>4.956041</td>\n",
       "      <td>5.242576</td>\n",
       "      <td>4.929656</td>\n",
       "      <td>45.746463</td>\n",
       "      <td>5.716792</td>\n",
       "      <td>5.489698</td>\n",
       "      <td>4.872688</td>\n",
       "      <td>5.353964</td>\n",
       "      <td>6.002283</td>\n",
       "      <td>5.655573</td>\n",
       "      <td>42.170790</td>\n",
       "      <td>5.331488</td>\n",
       "      <td>39.989426</td>\n",
       "      <td>5.384608</td>\n",
       "      <td>6.003897</td>\n",
       "      <td>49.078444</td>\n",
       "      <td>5.772318</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7199.000000</td>\n",
       "      <td>261.350410</td>\n",
       "      <td>32.566261</td>\n",
       "      <td>249.443153</td>\n",
       "      <td>29.926784</td>\n",
       "      <td>37.680869</td>\n",
       "      <td>28.115217</td>\n",
       "      <td>34.307626</td>\n",
       "      <td>269.012188</td>\n",
       "      <td>32.139129</td>\n",
       "      <td>30.009526</td>\n",
       "      <td>28.107796</td>\n",
       "      <td>31.056526</td>\n",
       "      <td>31.106671</td>\n",
       "      <td>32.352866</td>\n",
       "      <td>33.866680</td>\n",
       "      <td>29.035641</td>\n",
       "      <td>29.383438</td>\n",
       "      <td>205.582180</td>\n",
       "      <td>242.830133</td>\n",
       "      <td>230.490522</td>\n",
       "      <td>32.133845</td>\n",
       "      <td>25.827349</td>\n",
       "      <td>30.778288</td>\n",
       "      <td>242.602442</td>\n",
       "      <td>31.641473</td>\n",
       "      <td>31.332098</td>\n",
       "      <td>27.967060</td>\n",
       "      <td>275.387978</td>\n",
       "      <td>34.256647</td>\n",
       "      <td>26.527638</td>\n",
       "      <td>28.088836</td>\n",
       "      <td>32.279469</td>\n",
       "      <td>31.805931</td>\n",
       "      <td>32.543635</td>\n",
       "      <td>30.079199</td>\n",
       "      <td>28.484171</td>\n",
       "      <td>269.663397</td>\n",
       "      <td>254.174227</td>\n",
       "      <td>30.750204</td>\n",
       "      <td>...</td>\n",
       "      <td>30.745759</td>\n",
       "      <td>28.842747</td>\n",
       "      <td>35.435966</td>\n",
       "      <td>32.027693</td>\n",
       "      <td>28.491744</td>\n",
       "      <td>37.633854</td>\n",
       "      <td>28.665034</td>\n",
       "      <td>37.709767</td>\n",
       "      <td>30.723516</td>\n",
       "      <td>28.364438</td>\n",
       "      <td>30.144335</td>\n",
       "      <td>31.036534</td>\n",
       "      <td>31.669406</td>\n",
       "      <td>32.520946</td>\n",
       "      <td>29.830201</td>\n",
       "      <td>34.002754</td>\n",
       "      <td>33.063522</td>\n",
       "      <td>29.570728</td>\n",
       "      <td>28.736670</td>\n",
       "      <td>28.572317</td>\n",
       "      <td>30.817551</td>\n",
       "      <td>25.806350</td>\n",
       "      <td>27.993830</td>\n",
       "      <td>29.472912</td>\n",
       "      <td>26.603016</td>\n",
       "      <td>268.705743</td>\n",
       "      <td>28.476074</td>\n",
       "      <td>40.160241</td>\n",
       "      <td>32.613635</td>\n",
       "      <td>33.761691</td>\n",
       "      <td>36.079314</td>\n",
       "      <td>32.325529</td>\n",
       "      <td>243.347500</td>\n",
       "      <td>33.403021</td>\n",
       "      <td>223.734432</td>\n",
       "      <td>29.623627</td>\n",
       "      <td>30.877309</td>\n",
       "      <td>244.195967</td>\n",
       "      <td>28.088101</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 252 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id     feature0  ...   feature249        label\n",
       "count  7200.000000  7200.000000  ...  7200.000000  7200.000000\n",
       "mean   3599.500000     2.504684  ...     0.273164     2.009167\n",
       "std    2078.605302    67.755261  ...     8.282975     1.411923\n",
       "min       0.000000  -236.552276  ...   -29.377652     0.000000\n",
       "25%    1799.750000   -42.590075  ...    -5.408092     1.000000\n",
       "50%    3599.500000     2.920268  ...     0.182123     2.000000\n",
       "75%    5399.250000    48.558923  ...     5.772318     3.000000\n",
       "max    7199.000000   261.350410  ...    28.088101     4.000000\n",
       "\n",
       "[8 rows x 252 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_check.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zwt66ads-tl8"
   },
   "source": [
    "As we can observe from df_check.describe(),\n",
    "\n",
    "-> We have 250 features(independent variables) and one Ground Truth.\n",
    "\n",
    "-> Here I'm Dropping the \"id\" colum as it won't contribute in learning.\n",
    "\n",
    "-> The dataset contains negative values to so I'm using \"StandardScaler\" for   \"Normalizing\" the input data to the models.\n",
    "\n",
    "-> Also, there's no Categorical feature in our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SPIsae6gAUgi",
    "outputId": "9174cf4b-d3cb-4c09-f898-8fbd249cbd7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique classes in our dataset:  5\n"
     ]
    }
   ],
   "source": [
    "# Lets check number of classes in our Ground Truth\n",
    "y = df_check.iloc[:, df_check.shape[1]-1].values # Grabbing only the last column \n",
    "number_classes = np.unique(y).shape[0]\n",
    "print(\"Number of unique classes in our dataset: \",number_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "Xfc-WJwHAueT",
    "outputId": "0c853904-6823-4886-b8d2-65fa787ae007"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7200\n",
      "Class_0: 1419 Class_1: 1426 Class_2: 1485 Class_3: 1410 Class_4: 1460\n"
     ]
    }
   ],
   "source": [
    "# Lets check whether our dataset is balanced for all the classes\n",
    "ret0, ret1, ret2, ret3, ret4 = [], [], [], [], []\n",
    "i = 0\n",
    "while i < y.shape[0]:\n",
    "  if y[i] == 0: ret0.append(y[i])\n",
    "  elif y[i] == 1: ret1.append(y[i])\n",
    "  elif y[i] == 2: ret2.append(y[i])\n",
    "  elif y[i] == 3: ret3.append(y[i])\n",
    "  elif y[i] == 4: ret4.append(y[i])\n",
    "  i += 1\n",
    "\n",
    "print(i)\n",
    "print(\"Class_0:\",len(ret0),\"Class_1:\",len(ret1),\"Class_2:\",len(ret2),\"Class_3:\",\n",
    "      len(ret3),\"Class_4:\",len(ret4)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "95eSjIGgBP_5"
   },
   "source": [
    "The above observation concludes our dataset is well balanced for all the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_WnHvGjXBeeP"
   },
   "outputs": [],
   "source": [
    "np.random.seed(9) # To produce same result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4tPCtJWpBs91"
   },
   "source": [
    "I have experimented on Various models:\n",
    "\n",
    "1) Logistic Regression\n",
    "\n",
    "2) Support Vector Machine\n",
    "\n",
    "3) XGBoost\n",
    "\n",
    "4) K Nearest Neighbor\n",
    "\n",
    "5) Artificial Neural Network with 2 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "amIvTcGbazlD"
   },
   "outputs": [],
   "source": [
    "# Function of all the Models I have experimented on\n",
    "def Logistic_Regression_model(X_train, y_train,X_val,y_val):\n",
    "  \n",
    "  LR = LogisticRegression()\n",
    "  parameters = {'penalty': ['l2'], 'C' : [0.001, 0.01, 0.1, 1, 10, 100, 1000], \n",
    "                'multi_class' :['multinomial'], \n",
    "                'solver': ['saga','lbfgs','newton-cg']}\n",
    "  LR = GridSearchCV(LR, param_grid = parameters ,cv=5)\n",
    "  LR.fit(X_train, y_train)\n",
    "  score_train_acc = LR.score(X_train, y_train)\n",
    "  print(\"Training Accuracy\", score_train_acc)\n",
    "  score_val_acc = LR.score(X_val, y_val)\n",
    "  print(\"Validation Accuracy\", score_val_acc) \n",
    "  y_train_check = LR.predict(X_train)\n",
    "  print(\"**** TRAINING INFORMATION ****\")\n",
    "  print(\"Classification Report and Confusion Matrix on Training\")\n",
    "  print(classification_report(y_train, y_train_check))\n",
    "  print(confusion_matrix(y_train, y_train_check, labels=np.unique(y_train)))\n",
    "  print(\"------------------------------\")\n",
    "  print(\"**** VALIDATION INFORMATION ****\")\n",
    "  y_pred_LR = LR.predict(X_val)\n",
    "  print(\"Classification Report and Confusion Matrix on Validation\")\n",
    "  print(classification_report(y_val, y_pred_LR))\n",
    "  print(confusion_matrix(y_val, y_pred_LR, labels=np.unique(y_pred_LR)))\n",
    "  pkl_filename = \"LR_pickle_model.pkl\"\n",
    "  with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(LR, file)\n",
    "  return score_train_acc, score_val_acc\n",
    "\n",
    "def SVM_model(X_train, y_train,X_val,y_val):\n",
    "  svm = SVC(random_state = 47, C = 0.1, kernel='rbf', \n",
    "                  class_weight = 'balanced', decision_function_shape = \"ovr\")\n",
    "  parameters = {'C':[0.1, 0.001, 1],'decision_function_shape':['ovo', 'ovr']}\n",
    "  svm = GridSearchCV(svm, param_grid= parameters ,cv=5)\n",
    "  svm.fit(X_train, y_train)\n",
    "  score_train_acc = svm.score(X_train, y_train)\n",
    "  print(\"Training Accuracy\", score_train_acc)\n",
    "  score_val_acc = svm.score(X_val, y_val)\n",
    "  print(\"Validation Accuracy\", score_val_acc) \n",
    "  y_train_check = svm.predict(X_train)\n",
    "  print(\"**** TRAINING INFORMATION ****\")\n",
    "  print(\"Classification Report and Confusion Matrix on Training\")\n",
    "  print(classification_report(y_train, y_train_check))\n",
    "  print(confusion_matrix(y_train, y_train_check, labels=np.unique(y_train)))\n",
    "  print(\"------------------------------\")\n",
    "  print(\"**** VALIDATION INFORMATION ****\")\n",
    "  y_pred_SVM = svm.predict(X_val)\n",
    "  print(\"Classification Report and Confusion Matrix on Validation\")\n",
    "  print(classification_report(y_val, y_pred_SVM))\n",
    "  print(confusion_matrix(y_val, y_pred_SVM, labels=np.unique(y_val)))\n",
    "  pkl_filename = \"SVM_pickle_model.pkl\"\n",
    "  with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(svm, file)\n",
    "  return score_train_acc, score_val_acc\n",
    "\n",
    "def XGBoost_model(X_train, y_train,X_val,y_val):\n",
    "  XGB = xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=0.8, gamma=0.4,\n",
    "              learning_rate=0.1, max_delta_step=0, max_depth=6,\n",
    "              min_child_weight=3, missing=None, n_estimators=100, n_jobs=1,\n",
    "              nthread=None, objective='multi:softmax', random_state=0,\n",
    "              reg_alpha=1e-05, reg_lambda=1, scale_pos_weight=1, seed=2,\n",
    "              silent=None, subsample=0.8, verbosity=1)\n",
    "  XGB.fit(X_train, y_train)\n",
    "  score_train_acc = XGB.score(X_train, y_train)\n",
    "  print(\"Training Accuracy\", score_train_acc)\n",
    "  score_val_acc = XGB.score(X_val, y_val)\n",
    "  print(\"Validation Accuracy\", score_val_acc)  \n",
    "  y_train_check = XGB.predict(X_train)\n",
    "  print(\"**** TRAINING INFORMATION ****\")\n",
    "  print(\"Classification Report and Confusion Matrix on Training\")\n",
    "  print(classification_report(y_train, y_train_check))\n",
    "  print(confusion_matrix(y_train, y_train_check, labels=np.unique(y_train)))\n",
    "  print(\"------------------------------\")\n",
    "  print(\"**** VALIDATION INFORMATION ****\")\n",
    "  y_pred_XGB = XGB.predict(X_val)\n",
    "  print(\"Classification Report and Confusion Matrix on Validation\")\n",
    "  print(classification_report(y_val, y_pred_XGB))\n",
    "  print(confusion_matrix(y_val, y_pred_XGB, labels=np.unique(y_pred_XGB)))\n",
    "  pkl_filename = \"XGB_pickle_model.pkl\"\n",
    "  with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(XGB, file)\n",
    "  return score_train_acc, score_val_acc\n",
    "\n",
    "def KNN_model(X_train, y_train,X_val,y_val):\n",
    "  knn = KNeighborsClassifier(n_neighbors=5,weights='uniform',algorithm='auto')\n",
    "  knn.fit(X_train, y_train)\n",
    "  score_train_acc = knn.score(X_train, y_train)\n",
    "  print(\"Training Accuracy\", score_train_acc) \n",
    "  score_val_acc = knn.score(X_val, y_val)\n",
    "  print(\"Validation Accuracy\", score_val_acc)\n",
    "  y_train_check = knn.predict(X_train)\n",
    "  print(\"**** TRAINING INFORMATION ****\")\n",
    "  print(\"Classification Report and Confusion Matrix on Training\")\n",
    "  print(classification_report(y_train, y_train_check))\n",
    "  print(confusion_matrix(y_train, y_train_check, labels=np.unique(y_train)))\n",
    "  print(\"------------------------------\")\n",
    "  print(\"**** VALIDATION INFORMATION ****\")\n",
    "  y_pred_knn = knn.predict(X_val) \n",
    "  print(\"Classification Report and Confusion Matrix on Validation\")\n",
    "  print(classification_report(y_val, y_pred_knn))\n",
    "  print(confusion_matrix(y_val, y_pred_knn, labels=np.unique(y_pred_knn)))\n",
    "  pkl_filename = \"KNN_pickle_model.pkl\"\n",
    "  with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(knn, file)\n",
    "  return score_train_acc, score_val_acc\n",
    "\n",
    "def ANN(learning_rate):\n",
    "  adam = Adam(lr = learning_rate)\n",
    "  classifier = Sequential()\n",
    "  classifier.add(Dense(units = 128, activation = 'relu', \n",
    "                       kernel_initializer = 'random_uniform', input_dim = 250 ))\n",
    "  classifier.add(Dropout(0.4))\n",
    "  classifier.add(Dense(units = 128, activation = 'relu', \n",
    "                       kernel_initializer = 'random_uniform'))\n",
    "  classifier.add(Dropout(0.4))\n",
    "  classifier.add(Dense(units = 5, activation = 'softmax', \n",
    "                       kernel_initializer = 'random_uniform'))\n",
    "  classifier.compile(optimizer = adam, loss = 'sparse_categorical_crossentropy',\n",
    "                     metrics = ['sparse_categorical_accuracy'])\n",
    "  print(classifier.summary())\n",
    "  return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e_BQP9eRvquY"
   },
   "outputs": [],
   "source": [
    "def train_val_model(models):\n",
    "  print(\"The model user have provided for tarining:\", models)\n",
    "  df = pd.read_csv(\"train.csv\") # Creating DataFrame\n",
    "  print(\"DataFrame\",df.shape) # Checking the dimension\n",
    "  X = df.iloc[:,1:-1].values # Converting DataFrame to Numpy and ignoring \"id\"\n",
    "  print(\"X\",X.shape) # Checking the dimension\n",
    "  y = df.iloc[:, df.shape[1]-1].values # Converting DataFrame to Numpy for Ground Truth\n",
    "  print(y.shape) # Checking the dimension\n",
    "  # Normalizing the inpu\n",
    "  sc = StandardScaler() \n",
    "  X = sc.fit_transform(X) \n",
    "  learning_rate = 0.0001  # Used for Adam Optimizer\n",
    "  iterr = 100 # Number of Epochs\n",
    "  n_folds = 5 # Number of folds for Cross Validation\n",
    "  bsize = 32  # Batch Size\n",
    "  train_acc, val_acc = [], [] # Lists for storing accuracies on each fold\n",
    "\n",
    "  # Cross validation\n",
    "  kfold = KFold(n_splits=n_folds, shuffle=True, random_state=7)\n",
    "  hist = []\n",
    "  val_hist=[]\n",
    "  acc=[]\n",
    "  val_acc=[]\n",
    "  for train, val in kfold.split(X, y):\n",
    "    X_train, X_val = X[train], X[val] # Grabbing X_train, X_val for Models\n",
    "    y_train, y_val = y[train], y[val] # Grabbiing y_train, y_val for Models\n",
    "    if models == \"ANN\":\n",
    "        ann_model = ANN(learning_rate)\n",
    "        history = ann_model.fit(X_train, y_train,validation_data=(X_val,y_val), epochs=iterr, batch_size=bsize, verbose=1)\n",
    "        #print(history.history.keys())\n",
    "        hist.append(history.history['loss'])\n",
    "        val_hist.append(history.history['val_loss'])\n",
    "        acc.append(history.history['sparse_categorical_accuracy'])\n",
    "        val_acc.append(history.history['val_sparse_categorical_accuracy'])\n",
    "        plt.figure()\n",
    "        plt.subplot(121)\n",
    "        # For Plotting, X_axis: Number of Epochs, Y_axis = Loss\n",
    "        plt.title(\"Trainig Loss\")\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.subplot(122)\n",
    "        plt.title(\"Validation Loss\")\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.show()\n",
    "        yield hist,val_hist,acc,val_acc\n",
    "\n",
    "    elif models == \"LR\":\n",
    "        lr_model = Logistic_Regression_model(X_train, y_train,X_val,y_val)\n",
    "        train_acc.append(lr_model[0])\n",
    "        val_acc.append(lr_model[1])\n",
    "        #print(\"TA\",train_acc)\n",
    "        print(\"Training Accuracy is:\", sum(train_acc)/n_folds)\n",
    "        print(\"Validation Accuracy is:\", sum(val_acc)/n_folds)\n",
    "\n",
    "    elif models == \"SVM\":\n",
    "        svm_model = SVM_model(X_train, y_train,X_val,y_val)\n",
    "        train_acc.append(svm_model[0])\n",
    "        val_acc.append(svm_model[1])\n",
    "        #print(\"TA\",train_acc)\n",
    "        print(\"Train Accuracy is:\", sum(train_acc)/n_folds)\n",
    "        print(\"Validation Accuracy is:\", sum(val_acc)/n_folds)\n",
    "\n",
    "    elif models == \"XGB\":\n",
    "        xgb_model = XGBoost_model(X_train, y_train,X_val,y_val)\n",
    "        train_acc.append(xgb_model[0])\n",
    "        val_acc.append(xgb_model[1])\n",
    "        #print(\"TA\",train_acc)\n",
    "        print(\"Train Accuracy is:\", sum(train_acc)/n_folds)\n",
    "        print(\"Validation Accuracy is:\", sum(val_acc)/n_folds)\n",
    "\n",
    "    elif models == \"KNN\":\n",
    "        knn_model = KNN_model(X_train, y_train,X_val,y_val)\n",
    "        train_acc.append(knn_model[0])\n",
    "        val_acc.append(knn_model[1])\n",
    "        #print(\"TA\",train_acc)\n",
    "        print(\"Train Accuracy is:\", sum(train_acc)/n_folds)\n",
    "        print(\"Validation Accuracy is:\", sum(val_acc)/n_folds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4UPW4RGBF-OS"
   },
   "source": [
    "**Information on Training Models**\n",
    "\n",
    "-> I'm using KFold Crossvalidation and splitting the Trainig data into Training and Validation\n",
    "\n",
    "-> I have used GridSearchCV for best params\n",
    "\n",
    "-> Also, I have **Hypertuned** all the experimented models and observed accuracy, loss, precision, recall, f1_scores\n",
    "\n",
    "-> After observing all the experimented models I decided to use one model for Prediction as it has the least overfiting amongst all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "m7aKaRgAgW24",
    "outputId": "9d03b493-adb7-4555-d293-f13452316ed8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model user have provided for tarining: KNN\n",
      "DataFrame (7200, 252)\n",
      "X (7200, 250)\n",
      "(7200,)\n",
      "Training Accuracy 0.9673611111111111\n",
      "Validation Accuracy 0.9326388888888889\n",
      "**** TRAINING INFORMATION ****\n",
      "Classification Report and Confusion Matrix on Training\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      1097\n",
      "           1       0.94      0.99      0.96      1156\n",
      "           2       0.97      0.98      0.97      1200\n",
      "           3       0.99      0.93      0.96      1136\n",
      "           4       0.99      0.96      0.97      1171\n",
      "\n",
      "    accuracy                           0.97      5760\n",
      "   macro avg       0.97      0.97      0.97      5760\n",
      "weighted avg       0.97      0.97      0.97      5760\n",
      "\n",
      "[[1082    5    4    3    3]\n",
      " [   8 1139    3    3    3]\n",
      " [   3   18 1173    3    3]\n",
      " [  14   37   20 1057    8]\n",
      " [  14   14   15    7 1121]]\n",
      "------------------------------\n",
      "**** VALIDATION INFORMATION ****\n",
      "Classification Report and Confusion Matrix on Validation\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.96      0.94       322\n",
      "           1       0.88      0.96      0.92       270\n",
      "           2       0.90      0.94      0.92       285\n",
      "           3       0.98      0.87      0.92       274\n",
      "           4       0.98      0.92      0.95       289\n",
      "\n",
      "    accuracy                           0.93      1440\n",
      "   macro avg       0.94      0.93      0.93      1440\n",
      "weighted avg       0.94      0.93      0.93      1440\n",
      "\n",
      "[[309   6   4   3   0]\n",
      " [  6 259   4   0   1]\n",
      " [  3  10 269   2   1]\n",
      " [  9  12  11 239   3]\n",
      " [  6   6  10   0 267]]\n",
      "Train Accuracy is: 0.1934722222222222\n",
      "Validation Accuracy is: 0.1865277777777778\n",
      "Training Accuracy 0.9739583333333334\n",
      "Validation Accuracy 0.9375\n",
      "**** TRAINING INFORMATION ****\n",
      "Classification Report and Confusion Matrix on Training\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97      1133\n",
      "           1       0.97      0.98      0.97      1136\n",
      "           2       0.97      0.98      0.98      1168\n",
      "           3       0.99      0.95      0.97      1129\n",
      "           4       0.99      0.97      0.98      1194\n",
      "\n",
      "    accuracy                           0.97      5760\n",
      "   macro avg       0.97      0.97      0.97      5760\n",
      "weighted avg       0.97      0.97      0.97      5760\n",
      "\n",
      "[[1118    5    4    3    3]\n",
      " [   8 1114    7    3    4]\n",
      " [   6   10 1146    3    3]\n",
      " [  13   17   14 1078    7]\n",
      " [  16    8   11    5 1154]]\n",
      "------------------------------\n",
      "**** VALIDATION INFORMATION ****\n",
      "Classification Report and Confusion Matrix on Validation\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.96      0.93       286\n",
      "           1       0.92      0.93      0.92       290\n",
      "           2       0.94      0.92      0.93       317\n",
      "           3       0.98      0.91      0.95       281\n",
      "           4       0.96      0.96      0.96       266\n",
      "\n",
      "    accuracy                           0.94      1440\n",
      "   macro avg       0.94      0.94      0.94      1440\n",
      "weighted avg       0.94      0.94      0.94      1440\n",
      "\n",
      "[[275   6   2   1   2]\n",
      " [ 11 271   2   2   4]\n",
      " [  8  13 293   0   3]\n",
      " [  8   5   9 256   3]\n",
      " [  4   1   5   1 255]]\n",
      "Train Accuracy is: 0.38826388888888885\n",
      "Validation Accuracy is: 0.3740277777777778\n",
      "Training Accuracy 0.9666666666666667\n",
      "Validation Accuracy 0.9361111111111111\n",
      "**** TRAINING INFORMATION ****\n",
      "Classification Report and Confusion Matrix on Training\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.99      0.96      1131\n",
      "           1       0.95      0.98      0.96      1128\n",
      "           2       0.98      0.97      0.97      1216\n",
      "           3       0.99      0.94      0.96      1118\n",
      "           4       0.99      0.96      0.97      1167\n",
      "\n",
      "    accuracy                           0.97      5760\n",
      "   macro avg       0.97      0.97      0.97      5760\n",
      "weighted avg       0.97      0.97      0.97      5760\n",
      "\n",
      "[[1116    8    2    2    3]\n",
      " [  18 1100    4    4    2]\n",
      " [  12   15 1182    4    3]\n",
      " [  23   20   15 1052    8]\n",
      " [  25   12    8    4 1118]]\n",
      "------------------------------\n",
      "**** VALIDATION INFORMATION ****\n",
      "Classification Report and Confusion Matrix on Validation\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.98      0.93       288\n",
      "           1       0.92      0.94      0.93       298\n",
      "           2       0.96      0.96      0.96       269\n",
      "           3       0.97      0.87      0.92       292\n",
      "           4       0.95      0.94      0.95       293\n",
      "\n",
      "    accuracy                           0.94      1440\n",
      "   macro avg       0.94      0.94      0.94      1440\n",
      "weighted avg       0.94      0.94      0.94      1440\n",
      "\n",
      "[[281   2   1   0   4]\n",
      " [ 10 281   2   3   2]\n",
      " [  3   4 258   0   4]\n",
      " [ 12  18   5 253   4]\n",
      " [ 10   1   3   4 275]]\n",
      "Train Accuracy is: 0.5815972222222222\n",
      "Validation Accuracy is: 0.56125\n",
      "Training Accuracy 0.9651041666666667\n",
      "Validation Accuracy 0.9263888888888889\n",
      "**** TRAINING INFORMATION ****\n",
      "Classification Report and Confusion Matrix on Training\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97      1159\n",
      "           1       0.93      0.97      0.95      1135\n",
      "           2       0.97      0.97      0.97      1194\n",
      "           3       0.99      0.93      0.96      1123\n",
      "           4       0.99      0.96      0.98      1149\n",
      "\n",
      "    accuracy                           0.97      5760\n",
      "   macro avg       0.97      0.96      0.97      5760\n",
      "weighted avg       0.97      0.97      0.97      5760\n",
      "\n",
      "[[1143    9    2    2    3]\n",
      " [  17 1105    6    3    4]\n",
      " [   9   21 1161    1    2]\n",
      " [  21   34   20 1044    4]\n",
      " [  14   14   10    5 1106]]\n",
      "------------------------------\n",
      "**** VALIDATION INFORMATION ****\n",
      "Classification Report and Confusion Matrix on Validation\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.98      0.92       260\n",
      "           1       0.90      0.97      0.93       291\n",
      "           2       0.94      0.93      0.93       291\n",
      "           3       0.97      0.85      0.91       287\n",
      "           4       0.97      0.91      0.94       311\n",
      "\n",
      "    accuracy                           0.93      1440\n",
      "   macro avg       0.93      0.93      0.93      1440\n",
      "weighted avg       0.93      0.93      0.93      1440\n",
      "\n",
      "[[254   3   1   1   1]\n",
      " [  5 282   2   0   2]\n",
      " [  7   8 271   3   2]\n",
      " [ 13  16   9 244   5]\n",
      " [ 13   5   6   4 283]]\n",
      "Train Accuracy is: 0.7746180555555556\n",
      "Validation Accuracy is: 0.7465277777777779\n",
      "Training Accuracy 0.9670138888888888\n",
      "Validation Accuracy 0.9326388888888889\n",
      "**** TRAINING INFORMATION ****\n",
      "Classification Report and Confusion Matrix on Training\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.96      1156\n",
      "           1       0.95      0.97      0.96      1149\n",
      "           2       0.97      0.96      0.97      1162\n",
      "           3       0.98      0.94      0.96      1134\n",
      "           4       0.98      0.97      0.97      1159\n",
      "\n",
      "    accuracy                           0.97      5760\n",
      "   macro avg       0.97      0.97      0.97      5760\n",
      "weighted avg       0.97      0.97      0.97      5760\n",
      "\n",
      "[[1140    7    1    4    4]\n",
      " [  16 1120    5    3    5]\n",
      " [  11   22 1120    4    5]\n",
      " [  24   20   11 1071    8]\n",
      " [  16    5   12    7 1119]]\n",
      "------------------------------\n",
      "**** VALIDATION INFORMATION ****\n",
      "Classification Report and Confusion Matrix on Validation\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.98      0.93       263\n",
      "           1       0.89      0.95      0.92       277\n",
      "           2       0.95      0.98      0.97       323\n",
      "           3       0.96      0.89      0.92       276\n",
      "           4       0.98      0.86      0.92       301\n",
      "\n",
      "    accuracy                           0.93      1440\n",
      "   macro avg       0.93      0.93      0.93      1440\n",
      "weighted avg       0.94      0.93      0.93      1440\n",
      "\n",
      "[[257   3   1   2   0]\n",
      " [ 10 264   1   0   2]\n",
      " [  3   1 317   0   2]\n",
      " [  9  13   6 246   2]\n",
      " [ 13  14   7   8 259]]\n",
      "Train Accuracy is: 0.9680208333333333\n",
      "Validation Accuracy is: 0.9330555555555555\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  model = \"KNN\" # Depends on User, User can select models to experiment from the given list [\"LR\",\"SVM\",\"XGB\",\"KNN\",\"ANN\"]\n",
    "  if model == \"ANN\":\n",
    "    try:\n",
    "      hist,val_hist,acc,val_acc = train_val_model(models=model)\n",
    "    except Exception as e:\n",
    "      pass\n",
    "  else:\n",
    "    _ = train_val_model(models=model)\n",
    "    for el in _: print(el)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o3VcGfDiHMhS"
   },
   "source": [
    "**Finalizing Model**\n",
    "\n",
    "**Results**\n",
    "\n",
    "**Logistic Regression**\n",
    "\n",
    "Training Accuracy: 0.7929166666666667 \n",
    "\n",
    "Validation Accuracy is: 0.7455555555555555\n",
    "\n",
    "\n",
    "**SVM**\n",
    "\n",
    "Training Accuracy: 0.9930902777777778 \n",
    "\n",
    "Validation Accuracy is: 0.9823611111111111\n",
    "\n",
    "**XGB**\n",
    "\n",
    "Training Accuracy: 0.1 \n",
    "\n",
    "Validation Accuracy is: 0.8411\n",
    "\n",
    "**KNN**\n",
    "\n",
    "Training Accuracy: 0.9680208333333333 \n",
    "\n",
    "Validation Accuracy is: 0.9330555555555555\n",
    "\n",
    "**My Model**\n",
    "\n",
    "Training Accuracy: 0.98834 \n",
    "\n",
    "Validation Accuracy is: 0.96058\n",
    "\n",
    "-> After considering various factors as mentioned previously I used K Nearest Neighbors for predicting the ground truth on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "KnTVo0bLcMzy",
    "outputId": "d4a7a127-416f-4d33-86ff-176dd2605ffa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame (7200, 252)\n",
      "X_shape (7200, 250)\n",
      "df_test (4800, 251)\n",
      "X_test (4800, 250)\n"
     ]
    }
   ],
   "source": [
    "# Finalised Model\n",
    "\n",
    "# Training DataFrame\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "print(\"DataFrame\",df.shape)\n",
    "X = df.iloc[:,1:-1].values\n",
    "print(\"X_shape\",X.shape)\n",
    "y = df.iloc[:, df.shape[1]-1].values\n",
    "\n",
    "# Testing DataFrame\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "ret_X = df_test.iloc[:,0].values # This will be used for Prediction.csv\n",
    "#print(ret_X)\n",
    "print(\"df_test\",df_test.shape)\n",
    "\n",
    "X_test = df_test.iloc[:,1:].values\n",
    "print(\"X_test\",X_test.shape)\n",
    "\n",
    "# Normalizing the Data\n",
    "sc = StandardScaler()\n",
    "X_test = sc.fit_transform(X_test)\n",
    "X = sc.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3lEar93z-KOS"
   },
   "source": [
    "Use the following two consecutive blocks only if the trained model is not saved on the disck.\n",
    "\n",
    "I faced this problem when I was working on the Jupyter Notebook.\n",
    "\n",
    "Savig the model on Google Colab has no issues and you can directly dp prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uh3SDN-L2-kf"
   },
   "outputs": [],
   "source": [
    "# Only use when model is not being saved\n",
    "\"\"\"\n",
    "def KNN_model(X_train, y_train,X_val,y_val, X_test):\n",
    "  knn = KNeighborsClassifier(n_neighbors=5,weights='uniform',algorithm='auto')\n",
    "  knn.fit(X_train, y_train)\n",
    "  score_train_acc = knn.score(X_train, y_train)\n",
    "  print(\"Training Accuracy\", score_train_acc)\n",
    "  score_val_acc = knn.score(X_val, y_val)\n",
    "  print(\"Validating Accuracy\", score_val_acc)\n",
    "  y_pred_test = knn.predict(X_test)\n",
    "  return y_pred_test\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "5wQAk9T33GMv",
    "outputId": "e4d6a203-828a-4d74-eb5c-ffb3e9783546"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy 0.9670138888888888\n",
      "Validating Accuracy 0.9326388888888889\n",
      "4800\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=7)\n",
    "for train, val in kfold.split(X, y):\n",
    "    X_train, X_val = X[train], X[val]\n",
    "    y_train, y_val = y[train], y[val]\n",
    "    \n",
    "res = KNN_model(X_train, y_train,X_val,y_val, X_test)\n",
    "print(len(res)) # Sanity check to match test input data rows\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mu6tJZvz3Lbz",
    "outputId": "da53bdfb-0963-4b79-d3a3-3d9d1948fff3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.unique(res).shape[0] # Checking number of unique predicted class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EcKuYRyx6iFA"
   },
   "outputs": [],
   "source": [
    "#ret = np.column_stack((ret_X, res)) # Combining the \"id\" column and \"predicted class\" for Prediction.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "JfyrZrik6m0h",
    "outputId": "a85ae5ab-85aa-47cf-aec4-679863c82262"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7200     4]\n",
      " [ 7201     2]\n",
      " [ 7202     2]\n",
      " ...\n",
      " [11997     4]\n",
      " [11998     4]\n",
      " [11999     4]]\n"
     ]
    }
   ],
   "source": [
    "print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6IDyc9W23Rox"
   },
   "outputs": [],
   "source": [
    "# Saving the Prediction.csv  \n",
    "#pd.DataFrame(ret).to_csv(\"prediction.csv\", sep = \"|\", header = [\"id\", \"Predictions\"], index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eLKnA1g49OtK"
   },
   "outputs": [],
   "source": [
    "# Load from saved model from file\n",
    "with open(\"KNN_pickle_model.pkl\", 'rb') as file:\n",
    "    pickle_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Roa02QeN9Qsp"
   },
   "outputs": [],
   "source": [
    "y_predictions = pickle_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IeDvuegp9nbY",
    "outputId": "91164ab1-aae0-4897-e54d-8aa02762d065"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_predictions).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WMTmdPmU96Is"
   },
   "outputs": [],
   "source": [
    "ret = np.column_stack((ret_X, y_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "rF6MEhw--F2y",
    "outputId": "e67419cd-b0e9-4924-e122-c5d9b7f6bcc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7200     4]\n",
      " [ 7201     2]\n",
      " [ 7202     2]\n",
      " ...\n",
      " [11997     4]\n",
      " [11998     4]\n",
      " [11999     4]]\n",
      "4800\n"
     ]
    }
   ],
   "source": [
    "print(ret)\n",
    "print(len(ret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the Prediction.csv  \n",
    "pd.DataFrame(ret).to_csv(\"Predictions.csv\", sep = \"|\", header = [\"id\", \"label\"], index = False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Final_Infrrd_ai.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
